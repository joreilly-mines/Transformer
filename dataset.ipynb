{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(\n",
    "\ttext: str,\n",
    "\tallowed_punctuation: str = \"-.,;:!?()\\\"\" + \"\".join(str(x) for x in range(10)),\n",
    "\tpunctuation_convert: dict[str,str] = {'â€”': '-'},\n",
    "\tnumbers_allowed: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\t# replace some special characters which unicode won't normalize properly\n",
    "\tfor char, replacement in punctuation_convert.items():\n",
    "\t\ttext = text.replace(char, replacement)\n",
    "\n",
    "\t# if a line has \".jpg\" in it, remove that line (this is specific to Don Quixote)\n",
    "\ttext = '\\n'.join(\n",
    "\t\tline \n",
    "\t\tfor line in text.split('\\n')\n",
    "\t\tif '.jpg' not in line\n",
    "\t)\n",
    "\n",
    "\t# Normalize the string to decompose Unicode characters\n",
    "\ttext = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "\t# Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "\ttext = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\t# remove newlines and tabs\n",
    "\ttext = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "\t# put spaces around allowed punctuation\n",
    "\tfor char in allowed_punctuation:\n",
    "\t\ttext = text.replace(char, f' {char} ')\n",
    "\n",
    "\n",
    "\t# remove leading and trailing spaces\n",
    "\ttext = text.strip()\n",
    "\n",
    "\t# remove multiple spaces\n",
    "\twhile '  ' in text:\n",
    "\t\ttext = text.replace('  ', ' ')\n",
    "\n",
    "\n",
    "\t# remove all characters except (alphanumeric, allowed_punctuation, ' ')\n",
    "\ttext = ''.join(\n",
    "\t\t(\n",
    "\t\t\tchar \n",
    "\t\t\tif (\n",
    "\t\t\t\t(char.isalnum() and numbers_allowed) or (char.isalpha())\n",
    "\t\t\t\tor char in allowed_punctuation \n",
    "\t\t\t\tor char == ' '\n",
    "\t\t\t)\n",
    "\t\t\telse ' '\n",
    "\t\t)\n",
    "\t\tfor char in text \n",
    "\t)\n",
    "\n",
    "\t# convert to lowercase\n",
    "\ttext = text.lower()\n",
    "\n",
    "\ttext = text.strip()\n",
    "\n",
    "\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "\ttext: str,\n",
    "\tprocess: bool = False,\n",
    ") -> list[str]:\n",
    "\tif process:\n",
    "\t\ttext = process_text(text)\n",
    "\treturn [token for token in text.split(' ') if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(list_to_split, split_size):\n",
    "    sublists = []\n",
    "    for i in range(0, len(list_to_split), split_size):\n",
    "        i2 = i + split_size if i + split_size < len(list_to_split) else len(list_to_split)\n",
    "        sublists.append(list_to_split[i:i2])\n",
    "        \n",
    "    return sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 84...\n",
      "\t426785 characters read\n",
      "Getting book 15...\n",
      "\t1241025 characters read\n",
      "Getting book 18...\n",
      "\t1192776 characters read\n",
      "Getting book 82...\n",
      "\t1124986 characters read\n",
      "Getting book 996...\n",
      "\t2342262 characters read\n",
      "Getting book 2600...\n",
      "\t3273998 characters read\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_RAW: list[str] = get_many_books([84, 15, 18, 82, 996, 2600])\n",
    "DATA: str = \" \".join(process_text(x, allowed_punctuation=\"\", numbers_allowed=False) for x in DATA_RAW)\n",
    "\n",
    "#print(DATA[:1000])\n",
    "\n",
    "DATA_TOKENIZED: list[str] = tokenize(DATA)\n",
    "TOKEN_SET: set[str] = set(DATA_TOKENIZED)\n",
    "TOKEN_ALPHABETICAL: list[str] = sorted(list(TOKEN_SET))\n",
    "TOKEN_TO_INDEX: dict[str, int] = {token: i for i, token in enumerate(TOKEN_ALPHABETICAL)}\n",
    "#INDEX_TO_TOKEN: dict[int, str] = {i: token for i, token in enumerate(TOKEN_ALPHABETICAL)}\n",
    "MODEL_DATA: list[int] = [TOKEN_TO_INDEX[token] for token in DATA_TOKENIZED]\n",
    "MODEL_DATA_CHUNKS: list[list[int]] = split_list(MODEL_DATA, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35726"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKEN_SET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
