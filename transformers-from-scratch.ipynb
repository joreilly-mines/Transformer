{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from jaxtyping import Float, Int\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "# \"CUDA\" means a GPU device -- it makes the training much faster!\n",
    "DEVICE: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{DEVICE = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head in math\n",
    "\n",
    "$$\n",
    "\tA(X) = \\sigma\\bigg(X W_Q W_K^T X^T + M\\bigg) X W_V W_O^T\n",
    "$$\n",
    "\n",
    "- $W_Q, W_K, W_V, W_O$ can be made with `nn.Linear` and will all have dimension `d_model` $\\times$ `d_head`\n",
    "- $M$ is a lower triangular mask matrix, look up how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(n_context: int) -> Float[torch.Tensor, \"n_context n_context\"]:\n",
    "\t# this should return a `n_context, n_context` matrix,\n",
    "\t# with zeros below and on the diag, and `-float(\"inf\")` below\n",
    "\t# output = ...\n",
    "\tmask = np.full((n_context, n_context), -np.inf, dtype=np.float64)\n",
    "\tmask[np.tril_indices_from(mask)] = 0\n",
    "\treturn torch.Tensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6303488,)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: it's intentional that `n_context` is not in the `GPTConfig`\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "\t# default test values -- too small for a real language model, but big enough for testing\n",
    "\td_vocab: int = 40_000\n",
    "\td_model: int = 128\n",
    "\td_mlp: int = 512\n",
    "\tn_heads: int = 4\n",
    "\td_head: int = 32\n",
    "\tn_layers: int = 6\n",
    "\tact_fn: type[nn.Module] = nn.ReLU\n",
    "\n",
    "\t@property\n",
    "\tdef n_params(self) -> int:\n",
    "\t\t\"an estimate of the number of parameters\"\n",
    "\t\treturn (\n",
    "\t\t\tself.d_vocab * self.d_model # embeddings (and tied unembeddings)\n",
    "\t\t\t+ (\n",
    "\t\t\t\tself.d_model * self.d_mlp * 2 # mlp weights\n",
    "\t\t\t\t+ self.d_model + self.d_mlp # mlp bias\n",
    "\t\t\t\t+ self.n_heads * ( # number of heads\n",
    "\t\t\t\t\t4 * self.d_model * self.d_head # 4 because Q, K, O, V\n",
    "\t\t\t\t)\n",
    "\t\t\t) * self.n_layers, # for each layer\n",
    "\t\t)\n",
    "\t\n",
    "print(GPTConfig().n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the residual stream is `n_context` by `d_model`\n",
    "\n",
    "# this is the row-wise (last dimension) softmax of x\n",
    "# F.softmax(x, dim=-1)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg: GPTConfig = cfg\n",
    "  \n",
    "\t\tself.W_Q = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_K = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_V = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_O = nn.Linear(cfg.d_head, cfg.d_model, bias=False)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tn_context = x.shape[0]\n",
    "\t\tp1 = self.W_Q(x).to(DEVICE)\n",
    "\t\tp2 = self.W_K(x).transpose(0,1).to(DEVICE)\n",
    "\t\tM = create_mask(n_context).to(DEVICE)\n",
    "\t\tp3 = self.W_V(x).to(DEVICE)\n",
    "\t\tp4 = self.W_O(p3).to(DEVICE)\n",
    "\t\treturn F.softmax(p1 @ p2 + M, dim=-1) @ p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([10, 128])\n",
      "y.shape = torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "# TESTING CODE\n",
    "cfg = GPTConfig()\n",
    "attn_head = AttentionHead(cfg).to(DEVICE)\n",
    "seq_len: int = 10\n",
    "x = torch.randn(seq_len, cfg.d_model).to(DEVICE)\n",
    "print(f\"{x.shape = }\")\n",
    "y = attn_head(x)\n",
    "print(f\"{y.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.heads = nn.ModuleList([AttentionHead(cfg) for _ in range(self.cfg.n_heads)])\n",
    "\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tfor head in self.heads:\n",
    "\t\t\tx = x + head.forward(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.W_MD = nn.Linear(cfg.d_mlp, cfg.d_model, bias=True)\n",
    "\t\tself.W_MU = nn.Linear(cfg.d_model, cfg.d_mlp, bias=True)\n",
    "\t\tself.ReLU = self.cfg.act_fn()\n",
    "  \n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tp1 = self.W_MU(x).transpose(0,1)\n",
    "\t\tp2 = self.ReLU(p1.transpose(0,1))\n",
    "\t\tp3 = self.W_MD(p2)\n",
    "\t\treturn p3\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.MHA = MultiHeadedAttention(cfg)\n",
    "        self.MLP = MLP(cfg)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        x = x + self.MHA.forward(x)\n",
    "        x = x + self.MLP.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.transformerBlocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(self.cfg.n_layers)])\n",
    "\t\tself.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
    "\n",
    "\tdef forward(self, x: Int[torch.Tensor, \"n_context\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "\t\tembedded = self.embedding(x)\n",
    "\t\tfor transformerBlock in self.transformerBlocks:\n",
    "\t\t\tembedded = transformerBlock.forward(embedded)\n",
    "\t\tunembedded = (self.embedding.weight @ embedded.transpose(0,1)).transpose(0,1)\n",
    "\t\treturn unembedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([10])\n",
      "y.shape = torch.Size([10, 40000])\n"
     ]
    }
   ],
   "source": [
    "# TESTING CODE\n",
    "cfg = GPTConfig()\n",
    "transformer = Transformer(cfg).to(DEVICE)\n",
    "seq_len: int = 10\n",
    "x = torch.randint(1, 9999, (10,)).to(DEVICE)\n",
    "print(f\"{x.shape = }\")\n",
    "y = transformer(x)\n",
    "print(f\"{y.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(\n",
    "\ttext: str,\n",
    "\tallowed_punctuation: str = \"-.,;:!?()\\\"\" + \"\".join(str(x) for x in range(10)),\n",
    "\tpunctuation_convert: dict[str,str] = {'â€”': '-'},\n",
    "\tnumbers_allowed: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\t# replace some special characters which unicode won't normalize properly\n",
    "\tfor char, replacement in punctuation_convert.items():\n",
    "\t\ttext = text.replace(char, replacement)\n",
    "\n",
    "\t# if a line has \".jpg\" in it, remove that line (this is specific to Don Quixote)\n",
    "\ttext = '\\n'.join(\n",
    "\t\tline \n",
    "\t\tfor line in text.split('\\n')\n",
    "\t\tif '.jpg' not in line\n",
    "\t)\n",
    "\n",
    "\t# Normalize the string to decompose Unicode characters\n",
    "\ttext = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "\t# Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "\ttext = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\t# remove newlines and tabs\n",
    "\ttext = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "\t# put spaces around allowed punctuation\n",
    "\tfor char in allowed_punctuation:\n",
    "\t\ttext = text.replace(char, f' {char} ')\n",
    "\n",
    "\n",
    "\t# remove leading and trailing spaces\n",
    "\ttext = text.strip()\n",
    "\n",
    "\t# remove multiple spaces\n",
    "\twhile '  ' in text:\n",
    "\t\ttext = text.replace('  ', ' ')\n",
    "\n",
    "\n",
    "\t# remove all characters except (alphanumeric, allowed_punctuation, ' ')\n",
    "\ttext = ''.join(\n",
    "\t\t(\n",
    "\t\t\tchar \n",
    "\t\t\tif (\n",
    "\t\t\t\t(char.isalnum() and numbers_allowed) or (char.isalpha())\n",
    "\t\t\t\tor char in allowed_punctuation \n",
    "\t\t\t\tor char == ' '\n",
    "\t\t\t)\n",
    "\t\t\telse ' '\n",
    "\t\t)\n",
    "\t\tfor char in text \n",
    "\t)\n",
    "\n",
    "\t# convert to lowercase\n",
    "\ttext = text.lower()\n",
    "\n",
    "\ttext = text.strip()\n",
    "\n",
    "\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "\ttext: str,\n",
    "\tprocess: bool = False,\n",
    ") -> list[str]:\n",
    "\tif process:\n",
    "\t\ttext = process_text(text)\n",
    "\treturn [token for token in text.split(' ') if token]\n",
    "\n",
    "def split_list(list_to_split, split_size):\n",
    "    sublists = []\n",
    "    for i in range(0, len(list_to_split), split_size):\n",
    "        i2 = i + split_size if i + split_size < len(list_to_split) else len(list_to_split)\n",
    "        sublists.append(list_to_split[i:i2])\n",
    "        \n",
    "    return sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 84...\n",
      "\t426785 characters read\n",
      "Getting book 15...\n",
      "\t1241025 characters read\n",
      "Getting book 18...\n",
      "\t1192776 characters read\n",
      "Getting book 82...\n",
      "\t1124986 characters read\n",
      "Getting book 996...\n",
      "\t2342262 characters read\n",
      "Getting book 2600...\n",
      "\t3273998 characters read\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_RAW: list[str] = get_many_books([84, 15, 18, 82, 996, 2600])\n",
    "DATA: str = \" \".join(process_text(x, allowed_punctuation=\"\", numbers_allowed=False) for x in DATA_RAW)\n",
    "\n",
    "#print(DATA[:1000])\n",
    "\n",
    "DATA_TOKENIZED: list[str] = tokenize(DATA)\n",
    "TOKEN_SET: set[str] = set(DATA_TOKENIZED)\n",
    "TOKEN_ALPHABETICAL: list[str] = sorted(list(TOKEN_SET))\n",
    "TOKEN_TO_INDEX: dict[str, int] = {token: i for i, token in enumerate(TOKEN_ALPHABETICAL)}\n",
    "#INDEX_TO_TOKEN: dict[int, str] = {i: token for i, token in enumerate(TOKEN_ALPHABETICAL)}\n",
    "MODEL_DATA: list[int] = [TOKEN_TO_INDEX[token] for token in DATA_TOKENIZED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DATA_CHUNKS: list[list[int]] = split_list(MODEL_DATA, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.Transformer was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Transformer])` or the `torch.serialization.safe_globals([Transformer])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[1;32m---> 11\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m Transformer(cfg)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\n8web\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.Transformer was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Transformer])` or the `torch.serialization.safe_globals([Transformer])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "file_path = 'model.pt'\n",
    "\n",
    "\n",
    "#Training Loop\n",
    "losses = []\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    transformer = torch.load(file_path).to(DEVICE)\n",
    "else:\n",
    "    transformer = Transformer(cfg).to(DEVICE)\n",
    "\n",
    "dataset = MODEL_DATA_CHUNKS\n",
    "\n",
    "#for epoch in range(0,1):\n",
    "#epoch_loss = 0\n",
    "\n",
    "dataset_train = 50 # Dataset Training Size\n",
    "\n",
    "# Variables for Progress Tracking\n",
    "sample_i = 0\n",
    "progress_step = max(1, dataset_train // 100)\n",
    "print(\"Progress: 0%\")\n",
    "\n",
    "for sample in dataset[:dataset_train]:\n",
    "    sample = torch.tensor(sample, dtype=torch.long).to(DEVICE)\n",
    "    inputs = sample[:-1]#.to(DEVICE)\n",
    "    targets = sample[1:]#.to(DEVICE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    #print(loss.item())\n",
    "    \n",
    "    sample_i += 1\n",
    "    if sample_i % progress_step == 0 or sample_i == dataset_train:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Progress: {sample_i / dataset_train * 100:.0f}%\")\n",
    "\n",
    "    torch.save(transformer, file_path)\n",
    "\n",
    "\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
