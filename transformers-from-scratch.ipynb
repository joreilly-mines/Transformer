{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from jaxtyping import Float, Int\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head in math\n",
    "\n",
    "$$\n",
    "\tA(X) = \\sigma\\bigg(X W_Q W_K^T X^T + M\\bigg) X W_V W_O^T\n",
    "$$\n",
    "\n",
    "- $W_Q, W_K, W_V, W_O$ can be made with `nn.Linear` and will all have dimension `d_model` $\\times$ `d_head`\n",
    "- $M$ is a lower triangular mask matrix, look up how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(n_context: int) -> Float[torch.Tensor, \"n_context n_context\"]:\n",
    "\t# this should return a `n_context, n_context` matrix,\n",
    "\t# with zeros below and on the diag, and `-float(\"inf\")` below\n",
    "\t# output = ...\n",
    "\tmask = np.full((n_context, n_context), -np.inf, dtype=np.float64)\n",
    "\tmask[np.tril_indices_from(mask)] = 0\n",
    "\treturn torch.Tensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2463488,)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: it's intentional that `n_context` is not in the `GPTConfig`\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "\t# default test values -- too small for a real language model, but big enough for testing\n",
    "\td_vocab: int = 10_000\n",
    "\td_model: int = 128\n",
    "\td_mlp: int = 512\n",
    "\tn_heads: int = 4\n",
    "\td_head: int = 32\n",
    "\tn_layers: int = 6\n",
    "\tact_fn: type[nn.Module] = nn.ReLU\n",
    "\n",
    "\t@property\n",
    "\tdef n_params(self) -> int:\n",
    "\t\t\"an estimate of the number of parameters\"\n",
    "\t\treturn (\n",
    "\t\t\tself.d_vocab * self.d_model # embeddings (and tied unembeddings)\n",
    "\t\t\t+ (\n",
    "\t\t\t\tself.d_model * self.d_mlp * 2 # mlp weights\n",
    "\t\t\t\t+ self.d_model + self.d_mlp # mlp bias\n",
    "\t\t\t\t+ self.n_heads * ( # number of heads\n",
    "\t\t\t\t\t4 * self.d_model * self.d_head # 4 because Q, K, O, V\n",
    "\t\t\t\t)\n",
    "\t\t\t) * self.n_layers, # for each layer\n",
    "\t\t)\n",
    "\t\n",
    "print(GPTConfig().n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([10, 128])\n",
      "y.shape = torch.Size([10, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\levis\\AppData\\Local\\Temp\\ipykernel_46104\\3187171191.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(p1 @ p2 + M) @ p4\n"
     ]
    }
   ],
   "source": [
    "# note: the residual stream is `n_context` by `d_model`\n",
    "\n",
    "# this is the row-wise (last dimension) softmax of x\n",
    "# F.softmax(x, dim=-1)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg: GPTConfig = cfg\n",
    "  \n",
    "\t\tself.W_Q = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_K = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_V = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_O = nn.Linear(cfg.d_head, cfg.d_model, bias=False)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tn_context = x.shape[0]\n",
    "\t\tp1 = self.W_Q(x)\n",
    "\t\tp2 = self.W_K(x).transpose(0,1)\n",
    "\t\tM = create_mask(n_context)\n",
    "\t\tp3 = self.W_V(x)\n",
    "\t\tp4 = self.W_O(p3)\n",
    "\t\treturn F.softmax(p1 @ p2 + M) @ p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING CODE\n",
    "cfg = GPTConfig()\n",
    "attn_head = AttentionHead(cfg)\n",
    "seq_len: int = 10\n",
    "x = torch.randn(seq_len, cfg.d_model)\n",
    "print(f\"{x.shape = }\")\n",
    "y = attn_head(x)\n",
    "print(f\"{y.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.heads = nn.ModuleList([AttentionHead(cfg) for _ in range(self.cfg.n_heads)])\n",
    "\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tfor head in self.heads:\n",
    "\t\t\tx = x + head.forward(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.W_MD = nn.Linear(cfg.d_mlp, cfg.d_model, bias=True)\n",
    "\t\tself.W_MU = nn.Linear(cfg.d_model, cfg.d_mlp, bias=True)\n",
    "\t\tself.ReLU = self.cfg.act_fn()\n",
    "  \n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tp1 = self.W_MU(x).transpose(0,1)\n",
    "\t\tp2 = self.ReLU(p1.transpose(0,1))\n",
    "\t\tp3 = self.W_MD(p2)\n",
    "\t\treturn p3\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.MHA = MultiHeadedAttention(cfg)\n",
    "        self.MLP = MLP(cfg)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        x = x + self.MHA.forward(x)\n",
    "        x = x + self.MLP.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.transformerBlocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(self.cfg.n_layers)])\n",
    "\t\tself.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
    "\n",
    "\tdef forward(self, x: Int[torch.Tensor, \"n_context\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "\t\tembedded = self.embedding(x)\n",
    "\t\tfor transformerBlock in self.transformerBlocks:\n",
    "\t\t\tembedded = transformerBlock.forward(embedded)\n",
    "\t\tunembedded = (self.embedding.weight @ embedded.transpose(0,1)).transpose(0,1)\n",
    "\t\treturn unembedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([10])\n",
      "y.shape = torch.Size([10, 10000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joeyd\\AppData\\Local\\Temp\\ipykernel_31448\\3187171191.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(p1 @ p2 + M) @ p4\n"
     ]
    }
   ],
   "source": [
    "# TESTING CODE\n",
    "cfg = GPTConfig()\n",
    "transformer = Transformer(cfg)\n",
    "seq_len: int = 10\n",
    "x = torch.randint(1, 9999, (10,))\n",
    "print(f\"{x.shape = }\")\n",
    "y = transformer(x)\n",
    "print(f\"{y.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#for epoch in range(0,1):\n",
    "#epoch_loss = 0\n",
    "for sample in dataset:\n",
    "    inputs = sample[:-1]\n",
    "    targets = sample[1:]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
