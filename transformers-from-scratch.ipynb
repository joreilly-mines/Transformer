{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from jaxtyping import Float, Int\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"CUDA\" means a GPU device -- it makes the training much faster!\n",
    "DEVICE: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{DEVICE = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head in math\n",
    "\n",
    "$$\n",
    "\tA(X) = \\sigma\\bigg(X W_Q W_K^T X^T + M\\bigg) X W_V W_O^T\n",
    "$$\n",
    "\n",
    "- $W_Q, W_K, W_V, W_O$ can be made with `nn.Linear` and will all have dimension `d_model` $\\times$ `d_head`\n",
    "- $M$ is a lower triangular mask matrix, look up how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(n_context: int) -> Float[torch.Tensor, \"n_context n_context\"]:\n",
    "\t# this should return a `n_context, n_context` matrix,\n",
    "\t# with zeros below and on the diag, and `-float(\"inf\")` below\n",
    "\t# output = ...\n",
    "\tmask = np.full((n_context, n_context), -np.inf, dtype=np.float64)\n",
    "\tmask[np.tril_indices_from(mask)] = 0\n",
    "\treturn torch.Tensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: it's intentional that `n_context` is not in the `GPTConfig`\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "\t# default test values -- too small for a real language model, but big enough for testing\n",
    "\td_vocab: int = 40_000\n",
    "\td_model: int = 128\n",
    "\td_mlp: int = 512\n",
    "\tn_heads: int = 4\n",
    "\td_head: int = 32\n",
    "\tn_layers: int = 6\n",
    "\tact_fn: type[nn.Module] = nn.ReLU\n",
    "\n",
    "\t@property\n",
    "\tdef n_params(self) -> int:\n",
    "\t\t\"an estimate of the number of parameters\"\n",
    "\t\treturn (\n",
    "\t\t\tself.d_vocab * self.d_model # embeddings (and tied unembeddings)\n",
    "\t\t\t+ (\n",
    "\t\t\t\tself.d_model * self.d_mlp * 2 # mlp weights\n",
    "\t\t\t\t+ self.d_model + self.d_mlp # mlp bias\n",
    "\t\t\t\t+ self.n_heads * ( # number of heads\n",
    "\t\t\t\t\t4 * self.d_model * self.d_head # 4 because Q, K, O, V\n",
    "\t\t\t\t)\n",
    "\t\t\t) * self.n_layers, # for each layer\n",
    "\t\t)\n",
    "\t\n",
    "print(GPTConfig().n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the residual stream is `n_context` by `d_model`\n",
    "\n",
    "# this is the row-wise (last dimension) softmax of x\n",
    "# F.softmax(x, dim=-1)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg: GPTConfig = cfg\n",
    "  \n",
    "\t\tself.W_Q = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_K = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_V = nn.Linear(cfg.d_model, cfg.d_head, bias=False)\n",
    "\t\tself.W_O = nn.Linear(cfg.d_head, cfg.d_model, bias=False)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tn_context = x.shape[0]\n",
    "\t\tp1 = self.W_Q(x).to(DEVICE)\n",
    "\t\tp2 = self.W_K(x).transpose(0,1).to(DEVICE)\n",
    "\t\tM = create_mask(n_context).to(DEVICE)\n",
    "\t\tp3 = self.W_V(x).to(DEVICE)\n",
    "\t\tp4 = self.W_O(p3).to(DEVICE)\n",
    "\t\treturn F.softmax(p1 @ p2 + M, dim=-1) @ p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING CODE\n",
    "cfg = GPTConfig()\n",
    "attn_head = AttentionHead(cfg).to(DEVICE)\n",
    "seq_len: int = 10\n",
    "x = torch.randn(seq_len, cfg.d_model).to(DEVICE)\n",
    "print(f\"{x.shape = }\")\n",
    "y = attn_head(x)\n",
    "print(f\"{y.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.heads = nn.ModuleList([AttentionHead(cfg) for _ in range(self.cfg.n_heads)])\n",
    "\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tfor head in self.heads:\n",
    "\t\t\tx = x + head.forward(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.W_MD = nn.Linear(cfg.d_mlp, cfg.d_model, bias=True)\n",
    "\t\tself.W_MU = nn.Linear(cfg.d_model, cfg.d_mlp, bias=True)\n",
    "\t\tself.ReLU = self.cfg.act_fn()\n",
    "  \n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tp1 = self.W_MU(x).transpose(0,1)\n",
    "\t\tp2 = self.ReLU(p1.transpose(0,1))\n",
    "\t\tp3 = self.W_MD(p2)\n",
    "\t\treturn p3\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.MHA = MultiHeadedAttention(cfg)\n",
    "        self.MLP = MLP(cfg)\n",
    "        self.norm1 = nn.LayerNorm(cfg.d_model)\n",
    "        self.norm2 = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        x = x + self.MHA.forward(self.norm1(x))\n",
    "        x = x + self.MLP.forward(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\t\tself.transformerBlocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(self.cfg.n_layers)])\n",
    "\t\tself.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
    "\n",
    "\tdef forward(self, x: Int[torch.Tensor, \"n_context\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "\t\tn_context = x.shape[0]\n",
    "\t\tpos_embedding = nn.Embedding(n_context, cfg.d_model)\n",
    "\n",
    "\t\tpositions = torch.arange(n_context, device=x.device)\n",
    "\t\tpos_embedded = pos_embedding(positions)\n",
    "\n",
    "\t\ttoken_embedded = self.embedding(x)\n",
    "\t\tembedded = token_embedded + pos_embedded\n",
    "\t\tfor transformerBlock in self.transformerBlocks:\n",
    "\t\t\tembedded = transformerBlock.forward(embedded)\n",
    "\t\tunembedded = (self.embedding.weight @ embedded.transpose(0,1)).transpose(0,1)\n",
    "\t\treturn unembedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING CODE\n",
    "cfg = GPTConfig()\n",
    "transformer = Transformer(cfg).to(DEVICE)\n",
    "seq_len: int = 10\n",
    "x = torch.randint(1, 9999, (10,)).to(DEVICE)\n",
    "print(f\"{x.shape = }\")\n",
    "y = transformer(x)\n",
    "print(f\"{y.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(\n",
    "\ttext: str,\n",
    "\tallowed_punctuation: str = \"-.,;:!?()\\\"\" + \"\".join(str(x) for x in range(10)),\n",
    "\tpunctuation_convert: dict[str,str] = {'—': '-'},\n",
    "\tnumbers_allowed: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\t# replace some special characters which unicode won't normalize properly\n",
    "\tfor char, replacement in punctuation_convert.items():\n",
    "\t\ttext = text.replace(char, replacement)\n",
    "\n",
    "\t# if a line has \".jpg\" in it, remove that line (this is specific to Don Quixote)\n",
    "\ttext = '\\n'.join(\n",
    "\t\tline \n",
    "\t\tfor line in text.split('\\n')\n",
    "\t\tif '.jpg' not in line\n",
    "\t)\n",
    "\n",
    "\t# Normalize the string to decompose Unicode characters\n",
    "\ttext = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "\t# Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "\ttext = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\t# remove newlines and tabs\n",
    "\ttext = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "\t# put spaces around allowed punctuation\n",
    "\tfor char in allowed_punctuation:\n",
    "\t\ttext = text.replace(char, f' {char} ')\n",
    "\n",
    "\n",
    "\t# remove leading and trailing spaces\n",
    "\ttext = text.strip()\n",
    "\n",
    "\t# remove multiple spaces\n",
    "\twhile '  ' in text:\n",
    "\t\ttext = text.replace('  ', ' ')\n",
    "\n",
    "\n",
    "\t# remove all characters except (alphanumeric, allowed_punctuation, ' ')\n",
    "\ttext = ''.join(\n",
    "\t\t(\n",
    "\t\t\tchar \n",
    "\t\t\tif (\n",
    "\t\t\t\t(char.isalnum() and numbers_allowed) or (char.isalpha())\n",
    "\t\t\t\tor char in allowed_punctuation \n",
    "\t\t\t\tor char == ' '\n",
    "\t\t\t)\n",
    "\t\t\telse ' '\n",
    "\t\t)\n",
    "\t\tfor char in text \n",
    "\t)\n",
    "\n",
    "\t# convert to lowercase\n",
    "\ttext = text.lower()\n",
    "\n",
    "\ttext = text.strip()\n",
    "\n",
    "\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD TOKENIZATION\n",
    "\n",
    "# def tokenize(\n",
    "# \ttext: str,\n",
    "# \tprocess: bool = False,\n",
    "# ) -> list[str]:\n",
    "# \tif process:\n",
    "# \t\ttext = process_text(text)\n",
    "# \treturn [token for token in text.split(' ') if token]\n",
    "\n",
    "# def split_list(list_to_split, split_size):\n",
    "#     sublists = []\n",
    "#     for i in range(0, len(list_to_split), split_size):\n",
    "#         i2 = i + split_size if i + split_size < len(list_to_split) else len(list_to_split)\n",
    "#         sublists.append(list_to_split[i:i2])\n",
    "        \n",
    "#     return sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD TOKENIZATION\n",
    "\n",
    "# DATA_RAW: list[str] = get_many_books([84, 15, 18, 82, 996, 2600])\n",
    "# DATA: str = \" \".join(process_text(x, allowed_punctuation=\"\", numbers_allowed=False) for x in DATA_RAW)\n",
    "\n",
    "# #print(DATA[:1000])\n",
    "\n",
    "# DATA_TOKENIZED: list[str] = tokenize(DATA)\n",
    "# TOKEN_SET: set[str] = set(DATA_TOKENIZED)\n",
    "# TOKEN_ALPHABETICAL: list[str] = sorted(list(TOKEN_SET))\n",
    "# TOKEN_TO_INDEX: dict[str, int] = {token: i for i, token in enumerate(TOKEN_ALPHABETICAL)}\n",
    "# #INDEX_TO_TOKEN: dict[int, str] = {i: token for i, token in enumerate(TOKEN_ALPHABETICAL)}\n",
    "# MODEL_DATA: list[int] = [TOKEN_TO_INDEX[token] for token in DATA_TOKENIZED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW: list[str] = get_many_books([84, 15, 18, 82, 996, 2600])\n",
    "# Unsure how much of text processing is necessary for the pre-trained tokenizer\n",
    "DATA: str = \" \".join(process_text(x, allowed_punctuation=\"\", numbers_allowed=False) for x in DATA_RAW)\n",
    "\n",
    "TOKENIZER: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "MODEL_DATA: list[int] = TOKENIZER.encode(DATA)\n",
    "MODEL_DATA_CHUNKS: list[list[int]] = [MODEL_DATA[i:i+TOKENIZER.model_max_length] for i in range(0, len(MODEL_DATA), TOKENIZER.model_max_length)]\n",
    "\n",
    "# REFERENCE: TOKENIZER.decode() for decoding back to text\n",
    "# You also might have to pad input text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'model.pt'\n",
    "\n",
    "\n",
    "#Training Loop\n",
    "losses = []\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    transformer = torch.load(file_path).to(DEVICE)\n",
    "else:\n",
    "    transformer = Transformer(cfg).to(DEVICE)\n",
    "\n",
    "dataset = MODEL_DATA_CHUNKS\n",
    "\n",
    "#for epoch in range(0,1):\n",
    "#epoch_loss = 0\n",
    "\n",
    "dataset_train = 50 # Dataset Training Size\n",
    "\n",
    "# Variables for Progress Tracking\n",
    "sample_i = 0\n",
    "progress_step = max(1, dataset_train // 100)\n",
    "print(\"Progress: 0%\")\n",
    "\n",
    "for sample in dataset[:dataset_train]:\n",
    "    sample = torch.tensor(sample, dtype=torch.long).to(DEVICE)\n",
    "    inputs = sample[:-1]#.to(DEVICE)\n",
    "    targets = sample[1:]#.to(DEVICE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    #print(loss.item())\n",
    "    \n",
    "    sample_i += 1\n",
    "    if sample_i % progress_step == 0 or sample_i == dataset_train:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Progress: {sample_i / dataset_train * 100:.0f}%\")\n",
    "\n",
    "    torch.save(transformer, file_path)\n",
    "\n",
    "\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5ec1eddb181cd3e6f65fb0c2f7ff329e9883a9e6a878c4884d73b19a3ab3917"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
